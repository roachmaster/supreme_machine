version: "3.9"

services:
  llm:
    image: "my-ollama-llm"
    # This maps /home/lrocha/data/ollama_data on your host to /root/.ollama inside the container
    volumes:
      - /home/lrocha/data/ollama_data:/root/.ollama
    # Publishes port 11434 so you can reach Ollama from outside the container
    ports:
      - "11434:11434"
    # GPU reservations (applies mainly in Docker Swarm, ignored in plain Compose)
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              count: all
              capabilities:
                - "gpu"
    # Uncomment if you want the container to restart automatically.
    # restart: "unless-stopped"
